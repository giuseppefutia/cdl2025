{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuseppefutia/cdl2025/blob/master/vLLM_%2B_Gemma3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppSgVOr2TTHo"
      },
      "source": [
        "# Installation, Importing, Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBLJd3YXTRJr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qq fastapi uvicorn\n",
        "!pip install -qq vllm\n",
        "!pip install -qq pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a4Yj_9PAXoRb"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "# Standard library\n",
        "import os\n",
        "import socket\n",
        "import subprocess\n",
        "import sys\n",
        "import textwrap\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass\n",
        "from getpass import getpass\n",
        "from typing import Iterable, Generator, Optional, Sequence\n",
        "\n",
        "# Third-party\n",
        "import torch\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import JSONResponse, StreamingResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVawPCgZSuDc"
      },
      "outputs": [],
      "source": [
        "# You need to store your keys into Secret section\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "NGROK_TOKEN = userdata.get(\"NGROK_TOKEN\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "os.environ[\"NGROK_TOKEN\"] = NGROK_TOKEN\n",
        "\n",
        "login(HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vt2OGdjlXEMd"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken \"$NGROK_TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mizdlhnUXtG"
      },
      "outputs": [],
      "source": [
        "MODEL = 'google/medgemma-4b-it' # @param {type:'string'}\n",
        "VLLM_HOST = '0.0.0.0' # @param {type:'string'}\n",
        "VLLM_PORT = 8000 # @param {type:'integer'}\n",
        "API_HOST = '127.0.0.1' # @param {type:'string'}\n",
        "API_PORT = 8001 # @param {type:'integer'}\n",
        "MAX_MODEL_LEN = 131072 # @param {type:'integer'}\n",
        "# MAX_MODEL_LEN = 8192\n",
        "TENSOR_PARALLEL_SIZE = 1 # @param {type:'integer'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQDtSHSSk_Y"
      },
      "source": [
        "# Model Deployment with vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Dek8bldh1gX"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Config (simple, safe defaults for Colab)\n",
        "# ----------------------------\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Sequence, Generator, Tuple, List\n",
        "import subprocess, time, socket, torch\n",
        "from contextlib import contextmanager\n",
        "from threading import Thread\n",
        "\n",
        "@dataclass\n",
        "class VLLMConfig:\n",
        "    model: str = MODEL\n",
        "    host: str = VLLM_HOST\n",
        "    port: int = VLLM_PORT\n",
        "    max_model_len: int = MAX_MODEL_LEN\n",
        "    tensor_parallel_size: int = TENSOR_PARALLEL_SIZE\n",
        "    trust_remote_code: bool = True\n",
        "    download_dir: Optional[str] = None\n",
        "\n",
        "    @property\n",
        "    def base_url(self) -> str:\n",
        "        return f\"http://{self.host}:{self.port}\"\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Small helpers\n",
        "# ----------------------------\n",
        "\n",
        "def select_dtype() -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        major, _ = torch.cuda.get_device_capability(0)\n",
        "        return \"bfloat16\" if major >= 8 else \"float16\"\n",
        "    return \"float32\"\n",
        "\n",
        "def wait_for_port(host: str, port: int, timeout: float = 120.0, interval: float = 0.5) -> bool:\n",
        "    \"\"\"Very small readiness check without touching logs/files.\"\"\"\n",
        "    deadline = time.time() + timeout\n",
        "    while time.time() < deadline:\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.settimeout(1.0)\n",
        "            if s.connect_ex((host, port)) == 0:\n",
        "                return True\n",
        "        time.sleep(interval)\n",
        "    return False\n",
        "\n",
        "def build_args(cfg: VLLMConfig, dtype: str) -> Sequence[str]:\n",
        "    args = [\n",
        "        \"vllm\", \"serve\", cfg.model,\n",
        "        \"--dtype\", dtype,\n",
        "        \"--max-model-len\", str(cfg.max_model_len),\n",
        "        \"--tensor-parallel-size\", str(cfg.tensor_parallel_size),\n",
        "        \"--host\", cfg.host,\n",
        "        \"--port\", str(cfg.port),\n",
        "    ]\n",
        "    if cfg.trust_remote_code:\n",
        "        args.append(\"--trust-remote-code\")\n",
        "    if cfg.download_dir:\n",
        "        args += [\"--download-dir\", cfg.download_dir]\n",
        "    return args\n",
        "\n",
        "# ----------------------------\n",
        "# Log streaming helpers\n",
        "# ----------------------------\n",
        "\n",
        "def _pump(pipe, tag: str, sink: Optional[List[str]] = None):\n",
        "    \"\"\"Read a subprocess pipe line-by-line and stream it to stdout (and optional sink).\"\"\"\n",
        "    try:\n",
        "        for line in iter(pipe.readline, \"\"):\n",
        "            if not line:\n",
        "                break\n",
        "            print(f\"[vLLM {tag}] {line}\", end=\"\")   # stream to cell output\n",
        "            if sink is not None:\n",
        "                sink.append(f\"[{tag}] {line}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[vLLM LOG ERROR {tag}] {e}\")\n",
        "    finally:\n",
        "        try:\n",
        "            pipe.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "class VLLMProcess:\n",
        "    \"\"\"Wrapper for the vLLM subprocess that also carries in-memory logs.\"\"\"\n",
        "    def __init__(self, proc: subprocess.Popen):\n",
        "        self.proc = proc\n",
        "        self.logs: List[str] = []\n",
        "        self._t_out = Thread(target=_pump, args=(proc.stdout, \"OUT\", self.logs), daemon=True)\n",
        "        self._t_err = Thread(target=_pump, args=(proc.stderr, \"ERR\", self.logs), daemon=True)\n",
        "        self._t_out.start()\n",
        "        self._t_err.start()\n",
        "\n",
        "    @property\n",
        "    def pid(self) -> int:\n",
        "        return self.proc.pid\n",
        "\n",
        "    def wait(self, timeout: Optional[float] = None) -> int:\n",
        "        return self.proc.wait(timeout=timeout)\n",
        "\n",
        "# ----------------------------\n",
        "# Start/stop + context manager\n",
        "# ----------------------------\n",
        "\n",
        "def start_vllm(cfg: VLLMConfig) -> VLLMProcess:\n",
        "    dtype = select_dtype()\n",
        "    args = build_args(cfg, dtype)\n",
        "    proc = subprocess.Popen(\n",
        "        args,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "        encoding=\"utf-8\",\n",
        "        errors=\"replace\",\n",
        "        start_new_session=True,\n",
        "        bufsize=1,          # line-buffered for timely streaming\n",
        "    )\n",
        "    return VLLMProcess(proc)\n",
        "\n",
        "def stop_vllm(p: Optional[VLLMProcess]) -> None:\n",
        "    if not p:\n",
        "        return\n",
        "    proc = p.proc\n",
        "    try:\n",
        "        proc.terminate()\n",
        "        try:\n",
        "            proc.wait(timeout=10)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "@contextmanager\n",
        "def run_vllm(cfg: VLLMConfig) -> Generator[tuple[str, subprocess.Popen], None, None]:\n",
        "    \"\"\"\n",
        "    Minimal, Colab-safe: start vLLM, wait until reachable, yield (base_url, proc), then clean up.\n",
        "    No file logging, no extra dependencies.\n",
        "    \"\"\"\n",
        "    proc = start_vllm(cfg)\n",
        "    try:\n",
        "        if not wait_for_port(cfg.host, cfg.port, timeout=10000):\n",
        "            stop_vllm(proc)\n",
        "            raise RuntimeError(\"vLLM didn't become ready on time. (Port check failed)\")\n",
        "        print(f\"vLLM up at {cfg.base_url} (pid={proc.pid})\")\n",
        "        yield (cfg.base_url, proc)\n",
        "    finally:\n",
        "        stop_vllm(proc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIZjv7F6vRzG",
        "outputId": "4f6552aa-6d00-45b7-bed2-5937695d710e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[vLLM OUT] INFO 11-16 15:46:22 [__init__.py:216] Automatically detected platform cuda.\n",
            "[vLLM ERR] 2025-11-16 15:46:24.405135: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "[vLLM ERR] 2025-11-16 15:46:24.422473: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "[vLLM ERR] WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "[vLLM ERR] E0000 00:00:1763307984.444106    2419 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "[vLLM ERR] E0000 00:00:1763307984.450608    2419 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[vLLM ERR] W0000 00:00:1763307984.467124    2419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763307984.467148    2419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763307984.467150    2419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763307984.467152    2419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] 2025-11-16 15:46:24.472341: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "[vLLM ERR] To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:46:37 [api_server.py:1839] vLLM API server version 0.11.0\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:46:37 [utils.py:233] non-default args: {'model_tag': 'google/medgemma-4b-it', 'host': '0.0.0.0', 'model': 'google/medgemma-4b-it', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 131072}\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:46:56 [model.py:547] Resolved architecture: Gemma3ForConditionalGeneration\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:46:56 [model.py:1510] Using max model len 131072\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:46:58 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "[vLLM OUT] INFO 11-16 15:47:13 [__init__.py:216] Automatically detected platform cuda.\n",
            "[vLLM ERR] 2025-11-16 15:47:14.639233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "[vLLM ERR] WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "[vLLM ERR] E0000 00:00:1763308034.659949    2752 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "[vLLM ERR] E0000 00:00:1763308034.666306    2752 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[vLLM ERR] W0000 00:00:1763308034.681991    2752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763308034.682016    2752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763308034.682018    2752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763308034.682019    2752 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:47:23 [core.py:644] Waiting for init message from front-end.\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:47:23 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='google/medgemma-4b-it', speculative_config=None, tokenizer='google/medgemma-4b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/medgemma-4b-it, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:47:27 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m WARNING 11-16 15:47:28 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "[vLLM ERR] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:47:39 [gpu_model_runner.py:2602] Starting to load model google/medgemma-4b-it...\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:47:40 [gpu_model_runner.py:2634] Loading model from scratch...\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:47:40 [layer.py:444] MultiHeadAttention attn_backend: _Backend.XFORMERS, use_upstream_fa: False\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:47:40 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:47:41 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:48:00 [weight_utils.py:413] Time spent downloading weights for google/medgemma-4b-it: 19.536753 seconds\n",
            "[vLLM ERR] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m \n",
            "[vLLM ERR] Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "[vLLM ERR] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m \n",
            "[vLLM ERR] Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.41s/it]\n",
            "[vLLM ERR] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m \n",
            "[vLLM ERR] Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.24s/it]\n",
            "[vLLM ERR] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m \n",
            "[vLLM ERR] Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.26s/it]\n",
            "[vLLM ERR] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m \n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:48:03 [default_loader.py:267] Loading weights took 2.61 seconds\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:48:04 [gpu_model_runner.py:2653] Model loading took 8.5844 GiB and 23.521825 seconds\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:48:05 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 2048 tokens, and profiled with 7 image items of the maximum feature size.\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:48:16 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/8219fcea73/rank_0_0/backbone for vLLM's torch.compile\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:48:16 [backends.py:559] Dynamo bytecode transform time: 9.88 s\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:48:22 [backends.py:197] Cache the graph for dynamic shape for later use\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:48:58 [backends.py:218] Compiling a graph for dynamic shape takes 41.13 s\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:49:07 [monitor.py:34] torch.compile takes 51.01 s in total\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:49:09 [gpu_worker.py:298] Available KV cache memory: 60.25 GiB\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m WARNING 11-16 15:49:09 [kv_cache_utils.py:982] Add 1 padding layers, may waste at most 3.45% KV cache memory\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:49:09 [kv_cache_utils.py:1087] GPU KV cache size: 451,280 tokens\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:49:09 [kv_cache_utils.py:1091] Maximum concurrency for 131,072 tokens per request: 21.12x\n",
            "[vLLM ERR] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m \n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 2/67 [00:00<00:03, 18.45it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 4/67 [00:00<00:03, 19.04it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 6/67 [00:00<00:03, 19.16it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 8/67 [00:00<00:03, 19.39it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▋        | 11/67 [00:00<00:02, 19.70it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 13/67 [00:00<00:02, 19.66it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 15/67 [00:00<00:02, 19.60it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 17/67 [00:00<00:02, 19.61it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 19/67 [00:00<00:02, 19.60it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 21/67 [00:01<00:02, 19.65it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 23/67 [00:01<00:02, 19.55it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 25/67 [00:01<00:02, 19.48it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 27/67 [00:01<00:02, 19.34it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 29/67 [00:01<00:02, 17.58it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▋     | 31/67 [00:01<00:01, 18.18it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 33/67 [00:01<00:01, 18.57it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|█████▏    | 35/67 [00:01<00:01, 18.67it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▌    | 37/67 [00:01<00:01, 18.99it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 39/67 [00:02<00:01, 19.20it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 41/67 [00:02<00:01, 19.24it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 43/67 [00:02<00:01, 19.21it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 45/67 [00:02<00:01, 19.30it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|███████   | 47/67 [00:02<00:01, 19.32it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 49/67 [00:02<00:00, 19.38it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▌  | 51/67 [00:02<00:00, 19.23it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 53/67 [00:02<00:00, 19.19it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 55/67 [00:02<00:00, 19.04it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|████████▌ | 57/67 [00:02<00:00, 19.03it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 59/67 [00:03<00:00, 18.98it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 61/67 [00:03<00:00, 18.99it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 63/67 [00:03<00:00, 18.42it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 65/67 [00:03<00:00, 18.12it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 15.13it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 18.60it/s]\n",
            "[vLLM ERR] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m \n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:01, 18.38it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 19.57it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:01, 19.85it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 19.89it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:00<00:01, 19.92it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:00<00:01, 19.89it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 19.54it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:00<00:00, 19.55it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:01<00:00, 19.48it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:01<00:00, 19.34it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:01<00:00, 19.50it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:01<00:00, 19.51it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 19.63it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:01<00:00, 19.74it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 19.91it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 19.75it/s]\n",
            "[vLLM ERR] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 19.66it/s]\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:49:16 [gpu_model_runner.py:3480] Graph capturing finished in 7 secs, took 0.73 GiB\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=2752)\u001b[0;0m INFO 11-16 15:49:16 [core.py:210] init engine (profile, create kv cache, warmup model) took 71.78 seconds\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:19 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 197440\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [api_server.py:1634] Supported_tasks: ['generate']\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:34] Available routes are:\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /docs, Methods: HEAD, GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /redoc, Methods: HEAD, GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /health, Methods: GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /load, Methods: GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /ping, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /ping, Methods: GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /tokenize, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /detokenize, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/models, Methods: GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /version, Methods: GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/responses, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/chat/completions, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/completions, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/embeddings, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /pooling, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /classify, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /score, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/score, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/audio/translations, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /rerank, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v1/rerank, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /v2/rerank, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /invocations, Methods: POST\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:20 [launcher.py:42] Route: /metrics, Methods: GET\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO:     Started server process [2419]\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO:     Waiting for application startup.\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO:     Application startup complete.\n",
            "vLLM up at http://0.0.0.0:8000 (pid=2419)\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:29 [chat_utils.py:560] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO:     127.0.0.1:42560 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "Completion result: I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind.\n",
            "\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO 11-16 15:49:30 [launcher.py:99] Shutting down FastAPI HTTP server.\n",
            "[vLLM ERR] [rank0]:[W1116 15:49:30.458114875 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO:     Shutting down\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO:     Waiting for application shutdown.\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=2419)\u001b[0;0m INFO:     Application shutdown complete.\n"
          ]
        }
      ],
      "source": [
        "# Test\n",
        "cfg = VLLMConfig(model=MODEL, port=VLLM_PORT)\n",
        "with run_vllm(cfg) as (base_url, proc):\n",
        "    client = OpenAI(\n",
        "        api_key=\"EMPTY\",                      # vLLM ignores auth by default\n",
        "        base_url=\"http://localhost:8000/v1\",  # your vLLM endpoint\n",
        "    )\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "    print(\"Completion result:\", resp.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu1O5GX7FJfD"
      },
      "outputs": [],
      "source": [
        "# Model for OpenAI client\n",
        "cfg = VLLMConfig()\n",
        "proc = start_vllm(cfg)\n",
        "try:\n",
        "    assert wait_for_port(cfg.host, cfg.port, timeout=300), \"vLLM did not open the port\"\n",
        "    print(f\"vLLM up at {cfg.base_url}\")\n",
        "\n",
        "    HOSTNAME = \"chatcompletion-uncognizable-nilda.ngrok-free.dev\"\n",
        "    public_url = ngrok.connect(cfg.port, \"http\", domain=HOSTNAME).public_url\n",
        "    public_v1 = public_url.rstrip(\"/\") + \"/v1\"\n",
        "    print(\"Public endpoint:\", public_v1)\n",
        "\n",
        "    # Quick in-notebook sanity check\n",
        "    import requests\n",
        "    r = requests.get(cfg.base_url + \"/v1/models\", timeout=10)\n",
        "    print(\"Local /v1/models:\", r.status_code)\n",
        "\n",
        "    print(\"Serving... (leave this cell running)\")\n",
        "    while True:\n",
        "        pass\n",
        "\n",
        "finally:\n",
        "    try: ngrok.kill()\n",
        "    except Exception: pass\n",
        "    stop_vllm(proc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTHi48Gyeicx"
      },
      "source": [
        "# Model Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdBC4fiYenRI"
      },
      "outputs": [],
      "source": [
        "def make_openai_client(base_url: str, api_key: str = \"EMPTY\") -> OpenAI:\n",
        "    \"\"\"\n",
        "    vLLM exposes an OpenAI-compatible API. Unless you set --api-key in vLLM,\n",
        "    any token is accepted; 'EMPTY' is a common no-op.\n",
        "    \"\"\"\n",
        "    return OpenAI(api_key=api_key, base_url=base_url)\n",
        "\n",
        "\n",
        "def ask_model_sync(\n",
        "    client: OpenAI,\n",
        "    model: str,\n",
        "    messages: Iterable[dict],\n",
        "    **kwargs,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Blocking call that returns the assistant's final message content.\n",
        "    kwargs (optional): temperature, max_tokens, etc.\n",
        "    \"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=list(messages),\n",
        "        **kwargs,\n",
        "    )\n",
        "    # vLLM returns OpenAI-style choices\n",
        "    return resp.choices[0].message.content or \"\"\n",
        "\n",
        "\n",
        "def stream_model(\n",
        "    client: OpenAI,\n",
        "    model: str,\n",
        "    messages: Iterable[dict],\n",
        "    **kwargs,\n",
        ") -> Generator[str, None, None]:\n",
        "    \"\"\"\n",
        "    Streaming generator that yields tokens (strings). You can wrap this in\n",
        "    FastAPI's StreamingResponse for SSE-style delivery.\n",
        "    \"\"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=list(messages),\n",
        "        stream=True,\n",
        "        **kwargs,\n",
        "    )\n",
        "    for event in stream:\n",
        "        # OpenAI SDK stream returns chunks with delta content\n",
        "        delta = getattr(getattr(event, \"choices\", [None])[0], \"delta\", None)\n",
        "        if delta and delta.content:\n",
        "            yield delta.content\n",
        "    # Signal end if you’re doing SSE\n",
        "    # yield \"[DONE]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "780BmrscWe5G",
        "outputId": "11f32299-e8a4-46f7-f4a2-d4718d81212c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vLLM up at http://0.0.0.0:8000 (pid=2532)\n",
            "Answer: I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind. I am ready to assist you.\n",
            "\n",
            "\n",
            "Streaming:\n",
            "I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind. I am available as an open-weights model, which means my underlying code and parameters are accessible to the public. I am ready to assist you.\n",
            "\n",
            "[stream done]\n"
          ]
        }
      ],
      "source": [
        "cfg = VLLMConfig(model=MODEL, port=VLLM_PORT)\n",
        "with run_vllm(cfg) as (base_url, proc):\n",
        "\n",
        "  # Test\n",
        "  answer = ask_model_sync(\n",
        "            client,\n",
        "            model=cfg.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Who are you? Are you ready?\"}],\n",
        "            temperature=0,\n",
        "            max_tokens=128,\n",
        "        )\n",
        "  print(\"Answer:\", answer)\n",
        "\n",
        "  # Test streaming\n",
        "  print(\"\\nStreaming:\")\n",
        "  for tok in stream_model(\n",
        "      client,\n",
        "      model=cfg.model,\n",
        "      messages=[{\"role\": \"user\", \"content\": \"Who are you? Give me details. Are you ready?\"}],\n",
        "      temperature=0,\n",
        "  ):\n",
        "      print(tok, end=\"\", flush=True)\n",
        "  print(\"\\n[stream done]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxwjE37NS0aa"
      },
      "source": [
        "# FastAPI Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcX7oyl1kI5w"
      },
      "outputs": [],
      "source": [
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "def build_app(client: OpenAI, model_name: str) -> FastAPI:\n",
        "    app = FastAPI(title=\"vLLM (OpenAI SDK) proxy\")\n",
        "    app.add_middleware(\n",
        "      CORSMiddleware,\n",
        "      allow_origins=[\"*\"],  # tighten for production\n",
        "      allow_credentials=True,\n",
        "      allow_methods=[\"*\"],\n",
        "      allow_headers=[\"*\"],\n",
        "  )\n",
        "\n",
        "    @app.post(\"/ask\")\n",
        "    def ask(req: QuestionRequest):\n",
        "        answer = ask_model_sync(\n",
        "            client,\n",
        "            model=model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": req.question}],\n",
        "            temperature=0,\n",
        "        )\n",
        "        return JSONResponse({\"answer\": answer})\n",
        "\n",
        "    @app.post(\"/stream\")\n",
        "    def stream(req: QuestionRequest):\n",
        "        gen = stream_model(\n",
        "            client,\n",
        "            model=model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": req.question}],\n",
        "            temperature=0,\n",
        "        )\n",
        "\n",
        "        # SSE-like stream: prefix lines with \"data: \" if you need strict SSE\n",
        "        def sse():\n",
        "            for chunk in gen:\n",
        "                yield f\"data: {chunk}\\n\\n\"\n",
        "            yield \"data: [DONE]\\n\\n\"\n",
        "\n",
        "        return StreamingResponse(sse(), media_type=\"text/event-stream\")\n",
        "\n",
        "    return app\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTv1ijmQTkoo",
        "outputId": "b86f062d-5661-47f7-faea-47058075adb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vLLM up at http://0.0.0.0:8000 (pid=3155)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [324]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * ngrok tunnel \"https://b65cb04e9d05.ngrok.app\" -> \"127.0.0.1:8001\"\n",
            "INFO:     2a01:e11:5401:8500:2587:3369:aa11:7af8:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     2a01:e11:5401:8500:2587:3369:aa11:7af8:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     2a01:e11:5401:8500:2587:3369:aa11:7af8:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "cfg = VLLMConfig(model=MODEL, port=VLLM_PORT)\n",
        "with run_vllm(cfg) as (base_url, proc):\n",
        "  public_url = ngrok.connect(API_PORT).public_url\n",
        "  print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"{API_HOST}:{API_PORT}\\\"\")\n",
        "  app = build_app(client, cfg.model)\n",
        "  config = uvicorn.Config(app, host=API_HOST, port=API_PORT, log_level=\"info\")\n",
        "  server = uvicorn.Server(config)\n",
        "\n",
        "  await server.serve()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO8JsHu4cbXek8pC9tR94Eb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}