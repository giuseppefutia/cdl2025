{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuseppefutia/cdl2025/blob/master/vLLM_%2B_Qwen_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pszS1mqgK_d"
      },
      "source": [
        "# Installation, Importing, Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TXcCoe1Wflwo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qq fastapi uvicorn\n",
        "!pip install -qq vllm\n",
        "!pip install -qq pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ERFZcnwTgOcA"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "# Standard library\n",
        "import os\n",
        "import socket\n",
        "import subprocess\n",
        "import sys\n",
        "import textwrap\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass\n",
        "from getpass import getpass\n",
        "from typing import Iterable, Generator, Optional, Sequence\n",
        "\n",
        "# Third-party\n",
        "import torch\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import JSONResponse, StreamingResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdhSNQ0-n0Dj",
        "outputId": "82ffbb52-2188-4c84-b521-8a6f1882261c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "# You need to store your keys into Secret section\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "NGROK_TOKEN = userdata.get(\"NGROK_TOKEN\")\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "os.environ[\"NGROK_TOKEN\"] = NGROK_TOKEN\n",
        "\n",
        "login(HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoeDiGNTEdNK",
        "outputId": "ffb50b45-c8e3-4da8-ca07-c69c6afaf862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken \"$NGROK_TOKEN\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "93VA-eezgS_5"
      },
      "outputs": [],
      "source": [
        "MODEL = 'google/medgemma-4b-it' # @param {type:'string'}\n",
        "VLLM_HOST = '0.0.0.0' # @param {type:'string'}\n",
        "VLLM_PORT = 8000 # @param {type:'integer'}\n",
        "API_HOST = '127.0.0.1' # @param {type:'string'}\n",
        "API_PORT = 8001 # @param {type:'integer'}\n",
        "MAX_MODEL_LEN = 8192 # @param {type:'integer'}\n",
        "TENSOR_PARALLEL_SIZE = 1 # @param {type:'integer'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFKCG1SvgU7u"
      },
      "source": [
        "# Embedding Model Deployment with vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jJ8hcJsxgYRe"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Config (simple, safe defaults for Colab)\n",
        "# ----------------------------\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Sequence, Generator, Tuple, List\n",
        "import subprocess, time, socket, torch\n",
        "from contextlib import contextmanager\n",
        "from threading import Thread\n",
        "\n",
        "MODEL = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\n",
        "VLLM_HOST = \"0.0.0.0\"\n",
        "VLLM_PORT = 8000\n",
        "MAX_MODEL_LEN = 32768        # gte-Qwen2-7B-instruct supports 32k context\n",
        "TENSOR_PARALLEL_SIZE = 1\n",
        "\n",
        "@dataclass\n",
        "class VLLMConfig:\n",
        "    model: str = MODEL\n",
        "    host: str = VLLM_HOST\n",
        "    port: int = VLLM_PORT\n",
        "    max_model_len: int = MAX_MODEL_LEN\n",
        "    tensor_parallel_size: int = TENSOR_PARALLEL_SIZE\n",
        "    trust_remote_code: bool = True\n",
        "    download_dir: Optional[str] = None\n",
        "    task: str = \"embed\"                 # <- key change for embeddings\n",
        "    served_model_name: Optional[str] = None  # optional: alias exposed by server\n",
        "\n",
        "    @property\n",
        "    def base_url(self) -> str:\n",
        "        return f\"http://{self.host}:{self.port}\"\n",
        "\n",
        "# ----------------------------\n",
        "# Small helpers (no filesystem/log files)\n",
        "# ----------------------------\n",
        "def select_dtype() -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        major, _ = torch.cuda.get_device_capability(0)\n",
        "        return \"bfloat16\" if major >= 8 else \"float16\"\n",
        "    return \"float32\"\n",
        "\n",
        "def wait_for_port(host: str, port: int, timeout: float = 120.0, interval: float = 0.5) -> bool:\n",
        "    deadline = time.time() + timeout\n",
        "    while time.time() < deadline:\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.settimeout(1.0)\n",
        "            if s.connect_ex((host, port)) == 0:\n",
        "                return True\n",
        "        time.sleep(interval)\n",
        "    return False\n",
        "\n",
        "def build_args(cfg: VLLMConfig, dtype: str) -> Sequence[str]:\n",
        "    args = [\n",
        "        \"vllm\", \"serve\", cfg.model,\n",
        "        \"--task\", cfg.task,                        # <- tell vLLM this is an embedding model\n",
        "        \"--dtype\", dtype,\n",
        "        \"--max-model-len\", str(cfg.max_model_len),\n",
        "        \"--tensor-parallel-size\", str(cfg.tensor_parallel_size),\n",
        "        \"--host\", cfg.host,\n",
        "        \"--port\", str(cfg.port),\n",
        "    ]\n",
        "    if cfg.trust_remote_code:\n",
        "        args.append(\"--trust-remote-code\")\n",
        "    if cfg.download_dir:\n",
        "        args += [\"--download-dir\", cfg.download_dir]\n",
        "    if cfg.served_model_name:\n",
        "        args += [\"--served-model-name\", cfg.served_model_name]\n",
        "    return args\n",
        "\n",
        "# ----------------------------\n",
        "# Log streaming helpers\n",
        "# ----------------------------\n",
        "def _pump(pipe, tag: str, sink: Optional[List[str]] = None):\n",
        "    \"\"\"Read a subprocess pipe line-by-line and stream it to stdout (and optional sink).\"\"\"\n",
        "    try:\n",
        "        for line in iter(pipe.readline, \"\"):\n",
        "            if not line:\n",
        "                break\n",
        "            print(f\"[vLLM {tag}] {line}\", end=\"\")   # stream to cell output\n",
        "            if sink is not None:\n",
        "                sink.append(f\"[{tag}] {line}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[vLLM LOG ERROR {tag}] {e}\")\n",
        "    finally:\n",
        "        try:\n",
        "            pipe.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "class VLLMProcess:\n",
        "    \"\"\"Wrapper for the vLLM subprocess that also carries in-memory logs.\"\"\"\n",
        "    def __init__(self, proc: subprocess.Popen):\n",
        "        self.proc = proc\n",
        "        self.logs: List[str] = []\n",
        "        self._t_out = Thread(target=_pump, args=(proc.stdout, \"OUT\", self.logs), daemon=True)\n",
        "        self._t_err = Thread(target=_pump, args=(proc.stderr, \"ERR\", self.logs), daemon=True)\n",
        "        self._t_out.start()\n",
        "        self._t_err.start()\n",
        "\n",
        "    @property\n",
        "    def pid(self) -> int:\n",
        "        return self.proc.pid\n",
        "\n",
        "    def wait(self, timeout: Optional[float] = None) -> int:\n",
        "        return self.proc.wait(timeout=timeout)\n",
        "\n",
        "# ----------------------------\n",
        "# Start/stop + context manager (with live logs)\n",
        "# ----------------------------\n",
        "def start_vllm(cfg: VLLMConfig) -> VLLMProcess:\n",
        "    dtype = select_dtype()\n",
        "    args = build_args(cfg, dtype)\n",
        "    proc = subprocess.Popen(\n",
        "        args,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "        encoding=\"utf-8\",\n",
        "        errors=\"replace\",\n",
        "        start_new_session=True,\n",
        "        bufsize=1,          # line-buffered for timely streaming\n",
        "    )\n",
        "    return VLLMProcess(proc)\n",
        "\n",
        "def stop_vllm(p: Optional[VLLMProcess]) -> None:\n",
        "    if not p:\n",
        "        return\n",
        "    proc = p.proc\n",
        "    try:\n",
        "        proc.terminate()\n",
        "        try:\n",
        "            proc.wait(timeout=10)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "@contextmanager\n",
        "def run_vllm(cfg: VLLMConfig) -> Generator[Tuple[str, VLLMProcess], None, None]:\n",
        "    \"\"\"Start vLLM, stream logs live, yield (base_url, process), then clean up.\"\"\"\n",
        "    p = start_vllm(cfg)\n",
        "    try:\n",
        "        if not wait_for_port(cfg.host, cfg.port, timeout=600):\n",
        "            stop_vllm(p)\n",
        "            raise RuntimeError(\"vLLM didn't become ready on time. (Port check failed)\")\n",
        "        print(f\"vLLM up at {cfg.base_url} (pid={p.pid})\")\n",
        "        yield (cfg.base_url, p)\n",
        "    finally:\n",
        "        stop_vllm(p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsQYbOy8gm8q",
        "outputId": "e91c823f-96af-4a4c-d3ec-4dbd4d51f274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[vLLM ERR] 2025-11-20 07:35:52.133636: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "[vLLM ERR] 2025-11-20 07:35:52.151724: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "[vLLM ERR] WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "[vLLM ERR] E0000 00:00:1763624152.174065    1683 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "[vLLM ERR] E0000 00:00:1763624152.183589    1683 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[vLLM ERR] W0000 00:00:1763624152.200572    1683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763624152.200594    1683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763624152.200596    1683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763624152.200598    1683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] 2025-11-20 07:35:52.205466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "[vLLM ERR] To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[vLLM OUT] INFO 11-20 07:36:02 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "[vLLM OUT] WARNING 11-20 07:36:02 [argparse_utils.py:90] argument 'task' is deprecated\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m INFO 11-20 07:36:02 [api_server.py:1977] vLLM API server version 0.11.1\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m INFO 11-20 07:36:02 [utils.py:253] non-default args: {'model_tag': 'Alibaba-NLP/gte-Qwen2-7B-instruct', 'host': '0.0.0.0', 'model': 'Alibaba-NLP/gte-Qwen2-7B-instruct', 'task': 'embed', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 32768}\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m INFO 11-20 07:36:03 [config.py:896] Found sentence-transformers tokenize configuration.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m INFO 11-20 07:36:19 [model.py:631] Resolved architecture: Qwen2ForCausalLM\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m INFO 11-20 07:36:19 [config.py:784] Found sentence-transformers modules configuration.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m INFO 11-20 07:36:19 [config.py:815] Found pooling configuration.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m INFO 11-20 07:36:19 [model.py:1968] Downcasting torch.float32 to torch.bfloat16.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m INFO 11-20 07:36:19 [model.py:1745] Using max model len 32768\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m WARNING 11-20 07:36:21 [vllm.py:486] Pooling models do not support full cudagraphs. Overriding cudagraph_mode to PIECEWISE.\n",
            "[vLLM OUT] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m INFO 11-20 07:36:21 [vllm.py:582] Only models using causal attention support chunked prefill and prefix caching; disabling both.\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m A new version of the following files was downloaded from https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct:\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m - tokenization_qwen.py\n",
            "[vLLM ERR] \u001b[1;36m(APIServer pid=1683)\u001b[0;0m . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "[vLLM ERR] 2025-11-20 07:36:28.982363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "[vLLM ERR] WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "[vLLM ERR] E0000 00:00:1763624189.004367    1934 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "[vLLM ERR] E0000 00:00:1763624189.014173    1934 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[vLLM ERR] W0000 00:00:1763624189.032203    1934 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763624189.032231    1934 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763624189.032234    1934 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM ERR] W0000 00:00:1763624189.032236    1934 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=1934)\u001b[0;0m INFO 11-20 07:36:39 [core.py:93] Initializing a V1 LLM engine (v0.11.1) with config: model='Alibaba-NLP/gte-Qwen2-7B-instruct', speculative_config=None, tokenizer='Alibaba-NLP/gte-Qwen2-7B-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Alibaba-NLP/gte-Qwen2-7B-instruct, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=PoolerConfig(pooling_type='LAST', normalize=True, dimensions=None, enable_chunked_processing=None, max_embed_len=None, softmax=None, activation=None, use_activation=None, logit_bias=None, step_tag_id=None, returned_token_ids=None), compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=1934)\u001b[0;0m INFO 11-20 07:36:40 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.28.0.12:36699 backend=nccl\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] [Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=1934)\u001b[0;0m INFO 11-20 07:36:40 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=1934)\u001b[0;0m INFO 11-20 07:36:40 [gpu_model_runner.py:3255] Starting to load model Alibaba-NLP/gte-Qwen2-7B-instruct...\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=1934)\u001b[0;0m INFO 11-20 07:37:07 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLEX_ATTENTION']\n",
            "[vLLM OUT] \u001b[1;36m(EngineCore_DP0 pid=1934)\u001b[0;0m INFO 11-20 07:37:07 [cuda.py:427] Using FLASH_ATTN backend.\n"
          ]
        }
      ],
      "source": [
        "cfg = VLLMConfig(model=MODEL, port=VLLM_PORT)\n",
        "\n",
        "with run_vllm(cfg) as (base_url, p):  # p is VLLMProcess (has .proc, .pid, .logs)\n",
        "    print(\"Server is ready at:\", base_url)\n",
        "\n",
        "    # OpenAI-compatible client pointing to local vLLM\n",
        "    client = OpenAI(base_url=f\"{base_url}/v1\", api_key=\"EMPTY\")  # api_key is ignored by vLLM\n",
        "\n",
        "    # Use the served alias if you set one; otherwise the model id\n",
        "    served_name = cfg.served_model_name or cfg.model\n",
        "\n",
        "    out = client.embeddings.create(\n",
        "        model=served_name,  # e.g. \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\n",
        "        input=[\n",
        "            \"A cozy coffee shop with brick walls.\",\n",
        "            \"The capital of Italy is Rome.\"\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    print(len(out.data[0].embedding), len(out.data[1].embedding))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B4cojSlm6As"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, Union\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---------- Pydantic models ----------\n",
        "class EmbeddingRequest(BaseModel):\n",
        "    # Accept a single string or a list of strings; coerce to a list internally\n",
        "    input: Union[str, List[str]]\n",
        "    # Optional override of the model per-request; defaults to the app's model_name\n",
        "    model: Optional[str] = None\n",
        "    # If True, L2-normalize each embedding before returning\n",
        "    normalize: bool = False\n",
        "\n",
        "class MeanPoolRequest(EmbeddingRequest):\n",
        "    # Same fields; this endpoint returns a single mean-pooled vector\n",
        "    pass\n",
        "\n",
        "# ---------- App factory ----------\n",
        "def build_app(client: OpenAI, model_name: str) -> FastAPI:\n",
        "    \"\"\"\n",
        "    Build a FastAPI that proxies to a vLLM embeddings server via the\n",
        "    OpenAI-compatible SDK, e.g. client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\n",
        "    \"\"\"\n",
        "    app = FastAPI(title=\"vLLM Embeddings Proxy\")\n",
        "\n",
        "    app.add_middleware(\n",
        "        CORSMiddleware,\n",
        "        allow_origins=[\"*\"],   # tighten for production\n",
        "        allow_credentials=True,\n",
        "        allow_methods=[\"*\"],\n",
        "        allow_headers=[\"*\"],\n",
        "    )\n",
        "\n",
        "    @app.get(\"/healthz\")\n",
        "    def healthz():\n",
        "        return {\"status\": \"ok\", \"model\": model_name}\n",
        "\n",
        "    # -------- Embedding helpers --------\n",
        "    def _to_list(x: Union[str, List[str]]) -> List[str]:\n",
        "        return [x] if isinstance(x, str) else x\n",
        "\n",
        "    def _l2_normalize(vec: List[float]) -> List[float]:\n",
        "        # Avoid pulling in numpy just for this\n",
        "        s = sum(v * v for v in vec) ** 0.5\n",
        "        return [v / s if s > 0 else 0.0 for v in vec]\n",
        "\n",
        "    # -------- Endpoints --------\n",
        "    @app.post(\"/embed\")\n",
        "    def embed(req: EmbeddingRequest):\n",
        "        inputs: List[str] = _to_list(req.input)\n",
        "        mdl = req.model or model_name\n",
        "\n",
        "        resp = client.embeddings.create(\n",
        "            model=mdl,\n",
        "            input=inputs,\n",
        "        )\n",
        "\n",
        "        # vLLM follows OpenAI schema: resp.data is a list matching inputs order\n",
        "        vectors = [d.embedding for d in resp.data]\n",
        "        if req.normalize:\n",
        "            vectors = [_l2_normalize(v) for v in vectors]\n",
        "\n",
        "        payload = {\n",
        "            \"model\": resp.model,\n",
        "            \"num_inputs\": len(inputs),\n",
        "            \"embedding_dims\": len(vectors[0]) if vectors else 0,\n",
        "            \"data\": vectors,  # List[List[float]]\n",
        "        }\n",
        "        return JSONResponse(payload)\n",
        "\n",
        "    @app.post(\"/embed/mean\")\n",
        "    def embed_mean(req: MeanPoolRequest):\n",
        "        inputs: List[str] = _to_list(req.input)\n",
        "        mdl = req.model or model_name\n",
        "\n",
        "        resp = client.embeddings.create(\n",
        "            model=mdl,\n",
        "            input=inputs,\n",
        "        )\n",
        "\n",
        "        vectors = [d.embedding for d in resp.data]\n",
        "        # Mean-pool across inputs (dimension-wise)\n",
        "        if not vectors:\n",
        "            mean_vec: List[float] = []\n",
        "        else:\n",
        "            dims = len(vectors[0])\n",
        "            # Verify consistent dims (defensive)\n",
        "            assert all(len(v) == dims for v in vectors), \"Inconsistent embedding dimensions\"\n",
        "            mean_vec = [sum(v[i] for v in vectors) / len(vectors) for i in range(dims)]\n",
        "\n",
        "        if req.normalize and mean_vec:\n",
        "            mean_vec = _l2_normalize(mean_vec)\n",
        "\n",
        "        payload = {\n",
        "            \"model\": resp.model,\n",
        "            \"inputs\": len(inputs),\n",
        "            \"embedding_dims\": len(mean_vec) if mean_vec else (len(vectors[0]) if vectors else 0),\n",
        "            \"embedding\": mean_vec,  # List[float]\n",
        "        }\n",
        "        return JSONResponse(payload)\n",
        "\n",
        "    return app\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hnnM6dgXnYO3"
      },
      "outputs": [],
      "source": [
        "cfg = VLLMConfig(model=MODEL, port=VLLM_PORT)\n",
        "with run_vllm(cfg) as (base_url, proc):   # 'proc' is the VLLMProcess; keep the name as in your original\n",
        "\n",
        "    client = OpenAI(base_url=f\"{base_url}/v1\", api_key=\"EMPTY\")\n",
        "    model_name = cfg.served_model_name or cfg.model\n",
        "\n",
        "    public_url = ngrok.connect(API_PORT).public_url\n",
        "    # print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"{API_HOST}:{API_PORT}\\\"\")\n",
        "\n",
        "    app = build_app(client, model_name)\n",
        "    config = uvicorn.Config(app, host=API_HOST, port=API_PORT, log_level=\"info\")\n",
        "    server = uvicorn.Server(config)\n",
        "\n",
        "    await server.serve()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyP7rlgel1kUu1yMGqZxTDbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}